{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is associated with the manuscript \"Missing data imputation via Gaussian Processes: Indo-Aryan sound change,\" Department of Comparative Linguistics, University of Zurich, 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "from numpy import log,exp,mean\n",
    "from functools import reduce\n",
    "import pymc3 as pm\n",
    "import theano.tensor as tt\n",
    "import theano\n",
    "theano.config.gcc.cxxflags = \"-fbracket-depth=1000\"\n",
    "#THEANO_FLAGS = \"-fbracket-depth=1000\"\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data and alignment indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = []\n",
    "\n",
    "for l in open('cdial_stripped.csv','r'):\n",
    "    text.append(l.strip().split('\\t'))\n",
    "\n",
    "\n",
    "for i in range(len(text)):\n",
    "    for j in range(1,3):\n",
    "        text[i][j] = text[i][j].split()\n",
    "\n",
    "\n",
    "\n",
    "alignments = []\n",
    "for l in open('alignments.txt','r'):\n",
    "    alignments.append([int (i) for i in l.split()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function extracts sound changes from aligned sequences, along with associated variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ng = 2\n",
    "D = ng*2\n",
    "def generate_ngrams(ng):\n",
    "    change_counts = defaultdict(int)\n",
    "    lang_changes = defaultdict()\n",
    "    for i in range(len(text)):\n",
    "        lang = text[i][0]\n",
    "        if lang not in lang_changes.keys():\n",
    "            lang_changes[lang] = defaultdict()\n",
    "        x,y=text[i][2],text[i][1]\n",
    "        A = alignments[i]\n",
    "        for j in range(0,len(A)-ng):\n",
    "            before = []\n",
    "            after = []\n",
    "            for k in range(ng):\n",
    "                before.append(tuple(x[j+k:j+k+1]))\n",
    "                after.append(tuple(y[A[j+k]:A[j+k+1]]))\n",
    "            edit = tuple([tuple(before),tuple(after)])\n",
    "            change_counts[edit]+=1\n",
    "            if edit[0] not in lang_changes[lang].keys():\n",
    "                lang_changes[lang][edit[0]] = defaultdict(int)\n",
    "            lang_changes[lang][edit[0]][edit[1]]+=1\n",
    "\n",
    "    all_reflex = defaultdict(list)\n",
    "    for k in change_counts.keys():\n",
    "        if change_counts[k] > 10:\n",
    "            all_reflex[k[0]].append(k[1])\n",
    "            \n",
    "    reflex = defaultdict(list)\n",
    "    for k in all_reflex.keys():\n",
    "        if len(all_reflex[k]) > 1:\n",
    "            reflex[k] = all_reflex[k]\n",
    "            \n",
    "    reflex_list = list(reflex.keys())\n",
    "    change_list = [(k,v) for k in reflex_list for v in reflex[k]]\n",
    "    seg_list = [sorted(set([change[i][j] for change in change_list])) for i in range(2) for j in range(ng)]\n",
    "    seg_len = [len(s) for s in seg_list]\n",
    "    return(reflex,reflex_list,change_list,lang_changes,seg_list,seg_len)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function generates the pairwise segment dissimilarity matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dissimilarity():\n",
    "    feat_mat = {}\n",
    "    for line in open('feat_matrix.csv','r'):\n",
    "        l = line.split('\\t')\n",
    "        feat_mat[l[0]]=tuple(l[1:])\n",
    "\n",
    "    dissimilarity = {}\n",
    "    dissimilarity[((),())] = 1\n",
    "    for k in feat_mat.keys():\n",
    "        dissimilarity[(('#',),(k,))] = 6\n",
    "        dissimilarity[((k,),('#',))] = 6\n",
    "        dissimilarity[(('#',),('#',))] = 0\n",
    "        for l in feat_mat.keys():\n",
    "            dissim = 0\n",
    "            if feat_mat[k][0]!=feat_mat[l][0]:\n",
    "                dissim += 2\n",
    "            if feat_mat[k][1]!=feat_mat[l][1]:\n",
    "                dissim += 1\n",
    "            if feat_mat[k][2]!=feat_mat[l][2]:\n",
    "                dissim += 1\n",
    "            if feat_mat[k][3]!=feat_mat[l][3]:\n",
    "                dissim += 1\n",
    "            if feat_mat[k][4]!=feat_mat[l][4]:\n",
    "                dissim += 1\n",
    "            dissimilarity[((k,),(l,))] = dissim\n",
    "    return(dissimilarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function generates the pairwise element dissimilarity matrix for each \"dimension\" of a given pair of changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_matrix():\n",
    "    Rho = [np.zeros([len(s),len(s)]) for s in seg_list]\n",
    "    for i in range(ng):\n",
    "        for j in range(seg_len[i]):\n",
    "            for k in range(j+1,seg_len[i]):\n",
    "                d = dissimilarity[(seg_list[i][j],seg_list[i][k])]\n",
    "                Rho[i][j,k] = d\n",
    "                Rho[i][k,j] = d            \n",
    "            \n",
    "    for i in range(ng,len(seg_list)):\n",
    "        for j in range(seg_len[i]):\n",
    "            for k in range(j+1,seg_len[i]):\n",
    "                if len(seg_list[i][j]) > 0 and len(seg_list[i][k]) > 0:\n",
    "                    if len(seg_list[i][j]) > 1 and len(seg_list[i][k]) == 1:\n",
    "                        d = np.mean([dissimilarity[((r,),seg_list[i][k])] for r in seg_list[i][j]])\n",
    "                    if len(seg_list[i][j]) == 1 and len(seg_list[i][k]) > 1:\n",
    "                        d = np.mean([dissimilarity[(seg_list[i][j],(s,))] for s in seg_list[i][k]])\n",
    "                    if len(seg_list[i][j]) > 1 and len(seg_list[i][k]) > 1:\n",
    "                        d = np.mean([dissimilarity[((r,),(s,))] for r in seg_list[i][j] for s in seg_list[i][k]])\n",
    "                    if len(seg_list[i][j]) == 1 and len(seg_list[i][k]) == 1:\n",
    "                        d = dissimilarity[(seg_list[i][j],seg_list[i][k])]\n",
    "                else:\n",
    "                    d = 3\n",
    "                Rho[i][j,k] = d\n",
    "                Rho[i][k,j] = d\n",
    "    return(Rho)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function generates a training and held-out data set for a given language, along with associated variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_batch(lang):\n",
    "    reflex_list_lang = [k for k in reflex_list if k in lang_changes[lang].keys()]\n",
    "    change_list_lang = [(k,v) for k in reflex_list for v in reflex[k] if k in lang_changes[lang].keys()]\n",
    "    X = len([k for k in reflex_list_lang])\n",
    "    R = [len(reflex[k]) for k in reflex_list_lang] #index and length of each sound change distribution\n",
    "    M = sum([R[i] for i in range(X)])\n",
    "\n",
    "    C = min(100,int(X/3))\n",
    "    partition = [[0,R[0]]]+[[reduce(lambda x,y:x+y,R[:i]),reduce(lambda x,y:x+y,R[:i+1])] for i in range(1,len(R))]\n",
    "    sound_binaries = [np.zeros([M,s]) for s in seg_len]\n",
    "\n",
    "    for s in range(M):\n",
    "        for i in range(2):\n",
    "            for j in range(ng):\n",
    "                sound_binaries[i*ng+j][s,seg_list[i*ng+j].index(change_list_lang[s][i][j])] = 1\n",
    "    \n",
    "    hold_out = sorted(random.sample(range(X),C))\n",
    "    leave_in = sorted(random.sample([x for x in range(X) if x not in hold_out],C))\n",
    "    \n",
    "    #ranges of left in/held out changes\n",
    "    range_leave_in = [partition[i] for i in leave_in]\n",
    "    range_hold_out = [partition[i] for i in hold_out]\n",
    "\n",
    "    #indices\n",
    "    ind_leave_in = []\n",
    "    for p in range_leave_in:\n",
    "        for i in range(p[0],p[1]):\n",
    "            ind_leave_in.append(i)\n",
    "        \n",
    "    ind_hold_out = []\n",
    "    for p in range_hold_out:\n",
    "        for i in range(p[0],p[1]):\n",
    "            ind_hold_out.append(i)        \n",
    "        \n",
    "    R_leave_in = [R[i] for i in leave_in]\n",
    "    R_hold_out = [R[i] for i in hold_out]\n",
    "\n",
    "    N_leave_in = sum(R_leave_in)\n",
    "    N_hold_out = sum(R_hold_out)\n",
    "    \n",
    "    #absolute partitions, for softmax\n",
    "    part_leave_in = [[0,R_leave_in[0]]]+[[reduce(lambda x,y:x+y,R_leave_in[:i]),reduce(lambda x,y:x+y,R_leave_in[:i+1])] for i in range(1,len(R_leave_in))]\n",
    "    part_hold_out = [[0,R_hold_out[0]]]+[[reduce(lambda x,y:x+y,R_hold_out[:i]),reduce(lambda x,y:x+y,R_hold_out[:i+1])] for i in range(1,len(R_hold_out))]\n",
    "    \n",
    "    sound_binaries_leave_in = [sound_binaries[i][ind_leave_in,:] for i in range(ng*2)]\n",
    "    \n",
    "    change_list_leave_in = [(reflex_list_lang[i],v) for i in leave_in for v in reflex[reflex_list_lang[i]]]\n",
    "    change_list_hold_out = [(reflex_list_lang[i],v) for i in hold_out for v in reflex[reflex_list_lang[i]]]\n",
    "    S = len(change_list_leave_in)\n",
    "    T = len(change_list_hold_out)\n",
    "    \n",
    "    sound_count = np.zeros(S)\n",
    "    for j,c in enumerate(change_list_leave_in):\n",
    "        sound_count[j] = lang_changes[lang][c[0]][c[1]]\n",
    "        \n",
    "    sound_count_held_out = np.zeros(T)\n",
    "    for j,c in enumerate(change_list_hold_out):\n",
    "        sound_count_held_out[j] = lang_changes[lang][c[0]][c[1]]\n",
    "        \n",
    "    Sigma = np.array([np.dot(np.dot(sound_binaries_leave_in[i],Rho[i]),sound_binaries_leave_in[i].T) for i in range(ng*2)])\n",
    "    \n",
    "    sound_binaries_full = [sound_binaries[i][ind_hold_out+ind_leave_in,:] for i in range(ng*2)]\n",
    "    Sigma_full = np.array([np.dot(np.dot(sound_binaries_full[i],Rho[i]),sound_binaries_full[i].T) for i in range(ng*2)])\n",
    "    \n",
    "    return(sound_count,sound_count_held_out,Sigma,Sigma_full,N_leave_in,N_hold_out,part_leave_in,part_hold_out,R_leave_in,R_hold_out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate the data set, the dissimilarity matrix, and the dimension-level dissimilarity matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reflex,reflex_list,change_list,lang_changes,seg_list,seg_len = generate_ngrams(ng)\n",
    "dissimilarity = generate_dissimilarity()\n",
    "Rho = generate_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The languages we are interesting in are not commented out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langs = [#'ashk1246',\n",
    " 'assa1263',\n",
    "# 'awad1243',\n",
    "# 'bagh1251',\n",
    "# 'balk1252',\n",
    " 'beng1280',\n",
    "# 'bhad1241',\n",
    "# 'bhat1263',\n",
    "# 'bhoj1244',\n",
    "# 'braj1242',\n",
    "# 'brok1247',\n",
    "# 'carp1235',\n",
    "# 'cham1307',\n",
    "# 'chil1275',\n",
    "# 'chur1258',\n",
    "# 'dame1241',\n",
    "# 'dhiv1236',\n",
    "# 'dogr1250',\n",
    "# 'doma1258',\n",
    "# 'doma1260',\n",
    "# 'garh1243',\n",
    "# 'gawa1247',\n",
    "# 'gran1245',\n",
    " 'guja1252',\n",
    "# 'halb1244',\n",
    " 'hind1269',\n",
    "# 'indu1241',\n",
    "# 'jaun1243',\n",
    "# 'kach1277',\n",
    "# 'kala1372',\n",
    "# 'kala1373',\n",
    "# 'kalo1256',\n",
    "# 'kang1280',\n",
    " 'kash1277',\n",
    "# 'kati1270',\n",
    "# 'khet1238',\n",
    "# 'khow1242',\n",
    "# 'kohi1248',\n",
    " 'konk1267',\n",
    "# 'kull1236',\n",
    "# 'kuma1273',\n",
    "# 'loma1235',\n",
    "# 'maga1260',\n",
    "# 'maha1287',\n",
    "# 'maha1305',\n",
    " 'mait1250',\n",
    "# 'malv1243',\n",
    "# 'mand1409',\n",
    " 'mara1378',\n",
    "# 'marw1260',\n",
    " 'nepa1254',\n",
    "# 'nort2665',\n",
    "# 'nort2666',\n",
    " 'oriy1255',\n",
    "# 'paha1251',\n",
    "# 'pali1273',\n",
    "# 'pang1282',\n",
    " 'panj1256',\n",
    "# 'phal1254',\n",
    "# 'pras1239',\n",
    "# 'savi1242',\n",
    "# 'sera1259',\n",
    "# 'shin1264',\n",
    "# 'shum1235',\n",
    " 'sind1272',\n",
    " 'sinh1246',\n",
    "# 'sint1235',\n",
    "# 'sirm1239',\n",
    "# 'sout2671',\n",
    "# 'sout2672',\n",
    "# 'tira1253',\n",
    "# 'torw1241',\n",
    "# 'treg1243',\n",
    "# 'vlax1238',\n",
    "# 'waig1243',\n",
    "# 'wels1246',\n",
    "# 'west2386',\n",
    "# 'wota1240'\n",
    "             ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function evaluates the model log-likelihood:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logprob(phi):\n",
    "    def lprob(sounds):\n",
    "        lp = pm.math.logsumexp(tt.dot(sounds,tt.log(phi.T)))\n",
    "        return(lp)\n",
    "    return(lprob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function runs inference for a batch of training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(N_leave_in,Sigma,part_leave_in,sound_count):\n",
    "    model = pm.Model()\n",
    "\n",
    "    with model:\n",
    "        rho = [pm.Uniform('rho_%i'%d,.001,100000) for d in range(D)]            #length scale        \n",
    "        alpha = pm.Uniform('alpha',.001,100000)        #dispersion parameter       \n",
    "        sigma = pm.Uniform('sigma',.001,100000)        #s.d.        \n",
    "        psi = pm.MvNormal('psi',mu=[0]*N_leave_in,\n",
    "                       cov = tt.power(alpha,2)*tt.exp(-.5*tt.sum([tt.power(rho[d],-2)*tt.power(Sigma[d],2)\n",
    "                                               for d in range(D)]))+\n",
    "                      (np.eye(N_leave_in)*tt.power(sigma,2)),\n",
    "                     shape=N_leave_in)             #weights       \n",
    "        phi = tt.concatenate([tt.nnet.softmax(psi[part_leave_in[x][0]:part_leave_in[x][1]])[0] for x in range(len(R_leave_in))])        \n",
    "        target = pm.DensityDist('target',logprob(phi),observed=sound_count)        \n",
    "        inference = pm.ADVI()        \n",
    "        inference.fit(2000,obj_optimizer=pm.adam(learning_rate=.01,beta1=.8),\n",
    "                                        callbacks=[pm.callbacks.CheckParametersConvergence()])\n",
    "        trace = inference.approx.sample()\n",
    "    return(trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function uses a posterior sample to predict sound changes for held out data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictive(trace,Sigma_full,N_hold_out,N_leave_in,part_hold_out,R_hold_out,sound_count_held_out):\n",
    "    j = np.random.randint(500)\n",
    "\n",
    "    a = trace['psi'][j]\n",
    "    rho = [trace['rho_'+str(d)][j] for d in range(D)]\n",
    "    sigma = trace['sigma'][j]\n",
    "    alpha = trace['alpha'][j]\n",
    "    \n",
    "    Sigma_f = (tt.power(alpha,2)*tt.exp(-.5*tt.sum([tt.power(rho[d],-2)*tt.power(Sigma_full[d],2)\n",
    "                                               for d in range(D)]))+\n",
    "                      (np.eye(N_leave_in+N_hold_out)*tt.power(sigma,2))).eval()\n",
    "    \n",
    "    Sigma_11 = Sigma_f[:N_hold_out,:N_hold_out]\n",
    "    Sigma_22 = Sigma_f[N_hold_out:,N_hold_out:]\n",
    "    Sigma_12 = Sigma_f[:N_hold_out,N_hold_out:]\n",
    "    Sigma_21 = Sigma_f[N_hold_out:,:N_hold_out]\n",
    "    \n",
    "    invSigma_22 = np.linalg.inv(Sigma_22)\n",
    "    CDinv = np.dot(Sigma_12,invSigma_22)\n",
    "    \n",
    "    mu_1 = np.matrix(np.zeros(N_hold_out))\n",
    "    mu_2 = np.matrix(np.zeros(N_leave_in))\n",
    "    mu_bar = np.array((mu_1.transpose() + (CDinv*(np.matrix(a)-mu_2).transpose())).flatten())[0]\n",
    "    Sigma_bar = Sigma_11 - np.dot(CDinv,Sigma_21)\n",
    "    \n",
    "    psi_22 = np.random.multivariate_normal(mu_bar,Sigma_bar)\n",
    "    phi_22 = tt.concatenate([tt.nnet.softmax(psi_22[part_hold_out[x][0]:part_hold_out[x][1]])[0] for x in range(len(R_hold_out))]).eval()\n",
    "    \n",
    "    N = [sum(sound_count_held_out[part_hold_out[x][0]:part_hold_out[x][1]]) for x in range(len(R_hold_out))]\n",
    "    \n",
    "    Z = tt.concatenate([np.random.multinomial(N[x],phi_22[part_hold_out[x][0]:part_hold_out[x][1]]) for x in range(len(R_hold_out))]).eval()\n",
    "    \n",
    "    F_score = f1_score(Z,sound_count_held_out,average='micro')\n",
    "    f = open('f_score.txt','a')\n",
    "    print(lang,F_score,file=f)\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full inference and evaluation procedure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(100):\n",
    "    for lang in langs:\n",
    "        sound_count,sound_count_held_out,Sigma,Sigma_full,N_leave_in,N_hold_out,part_leave_in,part_hold_out,R_leave_in,R_hold_out = gen_batch(lang)\n",
    "        trace=run_model(N_leave_in,Sigma,part_leave_in,sound_count)\n",
    "        predictive(trace,Sigma_full,N_hold_out,N_leave_in,part_hold_out,R_hold_out,sound_count_held_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
